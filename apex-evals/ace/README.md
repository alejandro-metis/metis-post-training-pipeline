# ACE: AI Consumer Index

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)

An evaluation benchmark for testing grounded AI responses across multiple domains.

## What is ACE?

ACE (AI Consumer Index) is a comprehensive evaluation benchmark designed to test AI models with web search/grounding capabilities. It evaluates how well models generate recommendations backed by verifiable web sources across four consumer-focused domains.

**Key Features:**

- **Multi-Domain Testing**: Shopping, Food, Gaming, and DIY domains

## Supported Models

ACE supports the latest AI models from major providers:

### Google Gemini

- `gemini-2.5-pro` - Gemini 2.5 Pro with Google Search grounding
- `gemini-2.5-flash` - Gemini 2.5 Flash with Google Search grounding
- `gemini-3-pro` - Gemini 3 Pro with Google Search grounding

### OpenAI

- `gpt-5` - GPT-5 with web search
- `gpt-5.1` - GPT-5.1 with web search
- `o3` - O3 with web search
- `o3-pro` - O3 Pro with web search

### Anthropic

- `sonnet-4.5` - Claude Sonnet 4.5 with web search
- `opus-4.1` - Claude Opus 4.1 with web search
- `opus-4.5` - Claude Opus 4.5 with web search

## Project Architecture

### File Structure

```
ACE/
├── configs/                   # Configuration and provider abstraction
│   ├── config.py              # Centralized credential management
│   ├── model_providers.py     # Multi-provider abstraction layer
│   └── domain_config.py       # Domain-specific configurations
├── dataset/                   # CSV datasets
│   ├── ACE-Shopping.csv
│   ├── ACE-Food.csv
│   ├── ACE-Gaming.csv
│   └── ACE-DIY.csv
├── harness/                   # Core evaluation harness
│   ├── make-grounded-call.py  # Stage 1: Grounded API calls
│   ├── grounding-pipeline.py  # Stage 2: Scraping & mapping
│   ├── autograder.py          # Stage 3: Evaluation
│   └── helpers/               # YouTube, Reddit, purchase verification
├── pipeline/                  # Scripts to run the harness
│   ├── runner.py              # Run all tasks for a domain/model/run
│   ├── run_all_models.py      # Run all models in parallel
│   ├── init_from_dataset.py   # Initialize test cases from CSV
│   ├── test_single_task.py    # Test a single task
│   ├── clear_run.py           # Clear a specific run
│   ├── clear_all_runs.py      # Clear all runs
│   ├── regrade_task.py        # Re-grade a task
│   ├── export_results.py      # Export results to CSV
│   ├── supabase_reader.py     # Read test cases from Supabase
│   └── local_file_reader.py   # Read test cases from local files
├── results/                   # Output directory (see Results Structure below)
├── exported_results/          # CSV exports (generated by export_results.py)
├── supabase-setup/            # Optional database setup scripts
├── pyproject.toml             # Python dependencies
├── uv.lock                    # Lock file for uv package manager
├── setup_xurls.sh             # Setup script for xurls binary
└── LICENSE                    # MIT License
```

### Dataset Structure

The `dataset/` folder contains CSV files with evaluation tasks:

**Files:**

- `ACE-Shopping.csv` - Shopping domain (product recommendations)
- `ACE-Food.csv` - Food domain (recipes, restaurants)
- `ACE-Gaming.csv` - Gaming domain (game recommendations, strategies)
- `ACE-DIY.csv` - DIY domain (home improvement, repair guides)

**CSV Schema:**

Each CSV in `dataset/` contains the following columns:

- `Criterion ID` — *Primary Key* Unique identifier for each evaluation criterion within a task (integer)
- `Task ID` — Unique identifier for each task (integer)
- `Specified Prompt` — The user query or prompt that will be given to models
- `Criterion Type` — Indicates if the criterion is "grounding" or "non-grounding"
- `Description` — Brief description of the evaluation criterion (explains what is being measured)
- `Answer` — The reference answer used for comparison or grounding verification (may be empty for some domains and criteria)
- Shopping Domain only:
  - `Product/Shop` — Indicates whether the item in question is asking for "product" or a "shop". This column is only present in ACE-Shopping.csv.

## Dependencies

**Python**: 3.11+

**AI Providers**:

- `anthropic` - Anthropic Claude API
- `openai` - OpenAI API
- `google-genai` - Google Gemini API

**Services**:

- `firecrawl-py` - Web scraping (required)
- `Searchapi-io` - Youtube Transcript scraping (required)

**Database**:

- `supabase` - Database backend (optional, for easier result viewing)

**Utilities**:

- `requests`, `python-dotenv`, `tqdm`, `html2text`

**External Tool**:

- `xurls` - REQUIRED for URL extraction (Go-based tool)

## Installation & Setup

### Step 1: Install uv Package Manager

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### Step 2: Install Python Dependencies

**Option A: Using uv (Recommended)**

`uv` automatically creates and manages a virtual environment for you:

```bash
uv sync
uv pip install -e .

# Activate the virtual environment
source .venv/bin/activate  # On macOS/Linux
# OR
.venv\Scripts\activate     # On Windows
```

> **Note**: `uv sync` creates a virtual environment in `.venv/` and installs all dependencies. You must activate the environment before running pipeline scripts.

**Option B: Using traditional venv**

If you prefer the traditional approach:

```bash
# Create virtual environment
python3 -m venv .venv

# Activate virtual environment
source .venv/bin/activate  # On macOS/Linux
# OR
.venv\Scripts\activate     # On Windows

# Install dependencies
pip install -r requirements.txt

# Install project in editable mode
pip install -e .
```

> **Note**: With traditional venv, you must activate the environment before running scripts. The `pip install -e .` step is required for proper imports.

### Step 3: Install xurls (REQUIRED)

xurls is **required** for URL extraction from model responses.

**Prerequisites**: Go 1.22+

```bash
# Install Go if not already installed
# macOS:
brew install go

# Linux: Download from https://go.dev/dl/ or use your package manager

# Install xurls
./setup_xurls.sh
```

The script will install xurls to `~/go/bin/xurls`. Verify installation:

```bash
~/go/bin/xurls --version
```

### Step 4: Configure API Keys

Create a `.env` file in the project root:

**Required - At least one model provider:**

```bash
GEMINI_API_KEY=your-gemini-key        # For Gemini models
OPENAI_API_KEY=your-openai-key        # For OpenAI models
ANTHROPIC_API_KEY=your-anthropic-key  # For Anthropic models
```

**Required - Web scraping:**

```bash
FIRECRAWL_API_KEY=your-firecrawl-key
SEARCHAPI_API_KEY=your-searchapi-key  # YouTube transcripts (falls back if missing)
```

**Optional - Supabase (for easier result viewing):**

```bash
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-key
```

## Results Structure

All results are stored as JSON files in the `results/` directory:

```
results/
└── {provider}/
    └── {model}/
        └── {domain}/
            └── run_{N}/
                └── task_{ID}/
                    ├── 0_test_case.json           # Input: task prompt and criteria
                    ├── 1_grounded_response.json   # Model response with grounding metadata
                    ├── 2_scraped_sources.json     # Scraped web sources and mappings
                    └── 3_autograder_results.json  # Final scores and reasoning
```

**Example path:** `results/gemini/gemini-2.5-pro/Shopping/run_1/task_715/`

**File descriptions:**

| File                          | Description                                                            |
| ----------------------------- | ---------------------------------------------------------------------- |
| `0_test_case.json`          | Contains the task ID, prompt, domain, and list of criteria to evaluate |
| `1_grounded_response.json`  | Model's response text and direct grounding metadata from the API       |
| `2_scraped_sources.json`    | Scraped content from cited URLs, product-to-source mappings            |
| `3_autograder_results.json` | Per-criterion scores (-1, 0, or 1), reasoning, and score summary       |

**Task completion:** A task is considered **complete** when `3_autograder_results.json` exists.

## Common Parameters

These parameters are used across multiple pipeline commands:

| Parameter             | Description                                                                                                                                                  | Default  | Recommended                                                             |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------- | ----------------------------------------------------------------------- |
| `--run N`           | Run number (1-8). Used for**variance testing** - run the same evaluation multiple times to measure consistency. Each run stores results independently. | Required | Start with `1`, then run `2-8` for statistical analysis             |
| `--workers N`       | Number of parallel tasks to process simultaneously. Higher = faster but uses more API quota.                                                                 | `10`   | `10-20` for normal use, `50-100` for fast runs with high API limits |
| `--force`           | Re-run all tasks, even if they already have results. Without this flag, completed tasks are skipped.                                                         | Off      | Use when re-running after fixing bugs                                   |
| `--skip-autograder` | Only run stages 1-2 (API call + scraping), skip stage 3 (autograding). Useful for debugging.                                                                 | Off      | Use for testing API responses without grading                           |
| `--supabase`        | Write results to Supabase database in addition to local files. Requires Supabase setup.                                                                      | Off      | Use if you want SQL-queryable results                                   |
| `--overwrite`       | Overwrite existing data (used with init_from_dataset.py).                                                                                                    | Off      | Use when re-initializing after CSV changes                              |
| `--dry-run`         | Preview what would happen without making changes.                                                                                                            | Off      | Use to verify before large operations                                   |

**About run numbers:**

The benchmark supports runs 1-8 to enable variance testing. AI model responses can vary between calls, so running the same evaluation multiple times helps measure:

- Response consistency
- Score variance across runs
- Statistical significance of results

For quick testing, just use run `1`. For full benchmarking, run all 8 and analyze the distribution.

**Note on running at scale:**

When running many tasks in parallel (high `--workers` or using `run_all_models.py`), you may encounter:

- API rate limits from model providers
- Firecrawl quota exhaustion
- Timeout errors on slow responses

If you hit these issues, reduce `--workers` or run fewer models concurrently. The internal version of this harness includes additional infrastructure for running at scale (retry logic, rate limiting, queue management) that is not included in this open-source release.

## How the Pipeline Works

Each task goes through three stages:

### Stage 1: Grounded API Call

**Script**: `harness/make-grounded-call.py`

- Sends prompt to AI model with web search enabled
- Model generates response with citations to web sources
- Saves grounded response with metadata

### Stage 2: Scraping & Mapping

**Script**: `harness/grounding-pipeline.py`

- Scrapes all cited sources using Firecrawl API
- Extracts recommendations from response
- Maps recommendations to sources
- Handles YouTube transcripts and Reddit discussions

### Stage 3: Autograding

**Script**: `harness/autograder.py`

- Two-stage verification per criterion:
  - **Stage 1**: Verify claim in response text
  - **Stage 2**: Verify claim in grounded sources
- Generates scores and detailed reasoning

## Running Evaluations

### Step 1: Initialize Test Cases (Required First)

Before running any evaluations, initialize test case files from the CSV datasets:

```bash
python3 pipeline/init_from_dataset.py <domain> <model> [options]
```

**Arguments:**

- `domain` — `Shopping`, `Food`, `Gaming`, `DIY`, or `all`
- `model` — Model name (e.g., `gemini-2.5-pro`) or `all`

**Options:**

- `--overwrite` — Overwrite existing test case files
- `--dry-run` — Preview without making changes
- `--supabase` — Also write to Supabase database (optional)

**Examples:**

```bash
# Initialize Shopping domain for one model
python3 pipeline/init_from_dataset.py Shopping gemini-2.5-pro

# Initialize all domains for one model
python3 pipeline/init_from_dataset.py all gemini-2.5-pro

# Initialize everything (all domains, all models)
python3 pipeline/init_from_dataset.py all all
```

### Step 2: Run Evaluations

#### Option A: Run a Single Task

Test one task to verify setup:

```bash
python3 pipeline/test_single_task.py <task_id> <model> <run_number> [domain] [options]
```

**Arguments:**

- `task_id` — Task ID from the CSV dataset
- `model` — Model name (e.g., `gemini-2.5-pro`, `gpt-5`, `sonnet-4.5`)
- `run_number` — Run number (1-8) for variance testing
- `domain` — Domain name (default: `Shopping`)

**Options:**

- `--skip-autograder` — Only run grounding + scraping, skip evaluation
- `--supabase` — Also write results to Supabase database

**Examples:**

```bash
python3 pipeline/test_single_task.py 312 gemini-2.5-pro 1 Shopping
python3 pipeline/test_single_task.py 161 sonnet-4.5 1 Food
python3 pipeline/test_single_task.py 276 gpt-5 1 Gaming --skip-autograder
```

#### Option B: Run All Tasks for One or More Models

Run all tasks for a specific domain/model/run combination. Supports multiple models:

```bash
python3 pipeline/runner.py <domain> --model <model> [model2 ...] --run <run_number> [options]
```

**Arguments:**

- `domain` — `Shopping`, `Food`, `Gaming`, or `DIY`
- `--model` — Model name(s) - can specify one or multiple (required)
- `--run` — Run number 1-8 (required)

**Options:**

- `--workers N` — Parallel workers (default: 10)
- `--force` — Re-run all tasks, ignoring existing results
- `--skip-autograder` — Only run grounding + scraping
- `--supabase` — Also write results to Supabase database

**Examples:**

```bash
# Run single model
python3 pipeline/runner.py Shopping --model gemini-2.5-pro --run 1

# Run multiple specific models (not all 10)
python3 pipeline/runner.py Shopping --model gemini-2.5-pro gemini-2.5-flash gemini-3-pro --run 1

# Run with more parallelism
python3 pipeline/runner.py Food --model gpt-5 --run 1 --workers 20

# Force re-run all tasks (ignore existing results)
python3 pipeline/runner.py Gaming --model sonnet-4.5 --run 1 --force
```

#### Option C: Run All Models in Parallel per Domain

Run all 10 models simultaneously for a domain/run:

```bash
python3 pipeline/run_all_models.py <domain> <run_number> [options]
```

**Arguments:**

- `domain` — `Shopping`, `Food`, `Gaming`, or `DIY`
- `run_number` — Run number (1-8)

**Options:**

- `--workers N` — Workers per model (default: 100)
- `--runner-flags ...` — Additional flags to pass to each runner (e.g., `--supabase`, `--force`)

**Examples:**

```bash
# Run all models for Shopping domain, run 1
python3 pipeline/run_all_models.py Shopping 1

# Run with custom worker count
python3 pipeline/run_all_models.py Food 1 --workers 50

# Run all models with Supabase enabled
python3 pipeline/run_all_models.py Gaming 1 --runner-flags --supabase

# Force re-run all models
python3 pipeline/run_all_models.py DIY 1 --runner-flags --force
```

### Retry Behavior and Resuming Failed Runs

All run commands (`test_single_task.py`, `runner.py`, `run_all_models.py`) automatically track task completion and support resumption. If tasks fail due to API rate limits, timeouts, or other errors, simply re-run the same command to complete the remaining tasks.

**How completion is determined:**

- A task is **complete** when `3_autograder_results.json` exists in its output folder
- With `--skip-autograder`, completion is determined by `2_scraped_sources.json` instead
- Failed tasks (missing completion file) will be retried from the beginning of model response generation

**Resuming after failures:**

If a run is interrupted or some tasks fail, simply re-run the exact same command:

```bash
# Single task - re-run if it failed
python3 pipeline/test_single_task.py 312 gemini-2.5-pro 1 Shopping

# Single model - re-run to complete failed tasks
python3 pipeline/runner.py Shopping --model gemini-2.5-pro --run 1

# All models - re-run to complete failed tasks across all models
python3 pipeline/run_all_models.py Shopping 1
```

The pipeline will:

1. Check which tasks already have the completion file (`3_autograder_results.json`)
2. Skip completed tasks entirely
3. Retry incomplete tasks from scratch (new model API call)

**Force re-running all tasks:**

To re-run tasks that already completed (e.g., after fixing a bug):

```bash
python3 pipeline/runner.py Shopping --model gemini-2.5-pro --run 1 --force
python3 pipeline/run_all_models.py Shopping 1 --runner-flags --force
```

### Step 3: Manage Results (Optional)

#### Clear a Specific Run

```bash
python3 pipeline/clear_run.py <domain> <run_number> --model <model> [options]
```

**Options:**

- `--yes` / `-y` — Skip confirmation prompt
- `--supabase` — Also clear from Supabase database

**Examples:**

```bash
python3 pipeline/clear_run.py Shopping 1 --model gemini-2.5-pro
python3 pipeline/clear_run.py Shopping 1 --model gemini-2.5-pro --yes
```

#### Clear All Runs

```bash
python3 pipeline/clear_all_runs.py <domain> --model <model> [options]
```

**Options:**

- `--start-run N` — Start from this run number (default: 1)
- `--end-run N` — End at this run number (default: 8)
- `--confirm` — Require confirmation before starting
- `--supabase` — Also clear from Supabase database

**Examples:**

```bash
# Clear all runs 1-8
python3 pipeline/clear_all_runs.py Shopping --model gemini-2.5-pro

# Clear only runs 3-5
python3 pipeline/clear_all_runs.py Shopping --model gemini-2.5-pro --start-run 3 --end-run 5
```

#### Re-grade a Task

Re-run only the autograder for a task (without re-calling the model):

```bash
python3 pipeline/regrade_task.py <task_id> <domain> <model> <run_number> [options]
```

**Options:**

- `--dry-run` — Preview without saving changes
- `--supabase` — Also update Supabase database

**Examples:**

```bash
python3 pipeline/regrade_task.py 312 Shopping gemini-2.5-pro 1
python3 pipeline/regrade_task.py 312 Shopping gemini-2.5-pro 1 --dry-run
```

#### Export Results to CSV

Export all results from local JSON files to CSV format for analysis:

```bash
python3 pipeline/export_results.py [options]
```

**Options:**

- `--domain DOMAIN` — Export a single domain (`Shopping`, `Food`, `Gaming`, or `DIY`)
- `--domains DOMAIN [DOMAIN ...]` — Export multiple specific domains

**Examples:**

```bash
# Export all domains
python3 pipeline/export_results.py

# Export single domain
python3 pipeline/export_results.py --domain Shopping

# Export multiple domains
python3 pipeline/export_results.py --domains Shopping Food
```

Output files are written to `exported_results/` with one CSV per domain containing all models and runs.

> **Note:** After clearing runs, you must re-initialize test cases before running new evaluations:
>
> ```bash
> python3 pipeline/init_from_dataset.py <domain> <model>
> ```

## Quick Start

Complete workflow for testing gemini-2.5-pro on Shopping domain:

```bash
# 1. Setup (one-time)
uv sync
uv pip install -e .
./setup_xurls.sh
# Create .env with API keys

# 2. Activate virtual environment
source .venv/bin/activate

# 3. Initialize test cases
python3 pipeline/init_from_dataset.py Shopping gemini-2.5-pro

# 4. Test a single task first
python3 pipeline/test_single_task.py 312 gemini-2.5-pro 1 Shopping

# 5. Run all tasks for run 1
python3 pipeline/runner.py Shopping --model gemini-2.5-pro --run 1

# 6. Check results
ls results/gemini/gemini-2.5-pro/Shopping/run_1/
cat results/gemini/gemini-2.5-pro/Shopping/run_1/task_312/3_autograder_results.json
```

## Optional: Supabase Integration

For easier result viewing and querying across runs, you can optionally enable Supabase database storage. Results will be written to both local files AND the database.

**Benefits:**

- Query and filter results with SQL
- Compare scores across models and runs
- Track results over time
- Easier analysis than parsing JSON files

**Setup:**

1. Create a project at https://supabase.com
2. Add credentials to `.env`:
   ```bash
   SUPABASE_URL=https://your-project.supabase.co
   SUPABASE_KEY=your-anon-public-key
   ```
3. Run the SQL from `supabase-setup/create_tables.sql` in Supabase SQL Editor (creates empty tables)
4. Optionally run `supabase-setup/create_rls_policies.sql` for access control
5. **Populate the tables** by running init_from_dataset.py with `--supabase`:
   ```bash
   python3 pipeline/init_from_dataset.py all all --supabase
   ```

**Usage:**

Add `--supabase` flag to commands to enable database writes:

```bash
# Initialize (required first - populates database tables from CSV)
python3 pipeline/init_from_dataset.py Shopping gemini-2.5-pro --supabase

# Run evaluations
python3 pipeline/runner.py Shopping --model gemini-2.5-pro --run 1 --supabase
python3 pipeline/run_all_models.py Shopping 1 --runner-flags --supabase
```

## Troubleshooting

### xurls not found

- Ensure Go is installed and in PATH
- Run `./setup_xurls.sh`
- Check `~/go/bin/xurls` exists

### Firecrawl errors

- Verify FIRECRAWL_API_KEY is valid
- Check Firecrawl API quota

### Tasks failing repeatedly

- Check the `1_grounded_response.json` file for API errors
- Review `2_scraped_sources.json` for scraping failures
- Re-run the command to retry failed tasks automatically
